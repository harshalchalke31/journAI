{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Projects\\python\\mainenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Define paths for storing the FAISS index and document metadata\n",
    "data_path = \"./data/pdf_data\"\n",
    "faiss_index_path = \"./data/faiss_index.bin\"\n",
    "doc_metadata_path = \"./data/doc_metadata.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Projects\\\\python\\\\journAI'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define system prompts or context\n",
    "system_prompt = \"\"\"\n",
    "You are an intelligent Q&A assistant. Your role is to provide precise, accurate, and concise answers to user queries based on the context retrieved from the knowledge base. \n",
    "\n",
    "Guidelines:\n",
    "1. Use the retrieved context to answer questions; avoid adding unsupported information.\n",
    "2. If the context is insufficient, state: \"The provided context does not contain enough information.\"\n",
    "3. Be brief and professional in your responses.\n",
    "\n",
    "Your ultimate goal is to assist the user effectively.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 3. Define how the user query should be wrapped\n",
    "query_wrapper_prompt = SimpleInputPrompt(\n",
    "    \"Based on the following context, respond to the user's query:\\n\\n<|CONTEXT|>\\n\\n<|USER|>{query_str}\\n<|ASSISTANT|>\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af38b6e92dc409c8dabdaa79dea6936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# device_map = {\n",
    "#     \"transformer\": \"cuda:0\",  # Put most of the model on the GPU\n",
    "#     \"lm_head\": \"cpu\",  # Offload the output head to the CPU\n",
    "# }\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=2040,  # 4096,\n",
    "    max_new_tokens=128,  # 256,\n",
    "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",  # Auto-distribute model layers across devices\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,  # Use mixed precision for lower memory usage\n",
    "        \"load_in_8bit\": True,\n",
    "        \"llm_int8_enable_fp32_cpu_offload\": True  # Enable CPU offloading for unsupported modules\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing FAISS index and metadata.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1. Load documents from PDF files\n",
    "docs = SimpleDirectoryReader(\"./data/pdf_data\").load_data()\n",
    "\n",
    "# 2. Set up embeddings using SentenceTransformer\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# 3. Define a function to embed documents\n",
    "def embed_documents(documents, model):\n",
    "    return [model.encode(doc.text, show_progress_bar=True) for doc in documents]\n",
    "\n",
    "# 5. Initialize or load FAISS index and metadata\n",
    "if os.path.exists(faiss_index_path) and os.path.exists(doc_metadata_path):\n",
    "    print(\"Loading existing FAISS index and metadata.\")\n",
    "    \n",
    "    # Load FAISS index\n",
    "    faiss_index = faiss.read_index(faiss_index_path)\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(doc_metadata_path, \"rb\") as f:\n",
    "        metadata = pickle.load(f)\n",
    "else:\n",
    "    print(\"FAISS index or metadata not found. Creating a new index.\")\n",
    "    \n",
    "    # Embed documents and convert to NumPy array\n",
    "    embeddings = embed_documents(docs, embed_model)\n",
    "    embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]  # Embedding dimension\n",
    "    faiss_index = faiss.IndexFlatL2(dimension)  # L2 distance index\n",
    "    faiss_index.add(embeddings)\n",
    "\n",
    "    # Save FAISS index\n",
    "    faiss.write_index(faiss_index, faiss_index_path)\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = [{\"text\": doc.text, \"id\": i} for i, doc in enumerate(docs)]\n",
    "    with open(doc_metadata_path, \"wb\") as f:\n",
    "        pickle.dump(metadata, f)\n",
    "\n",
    "\n",
    "# 6. Function to query the FAISS index\n",
    "def query_index(query, model, faiss_index, metadata, top_k=5):\n",
    "    query_vector = model.encode(query)\n",
    "    distances, indices = faiss_index.search(query_vector.reshape(1, -1), top_k)\n",
    "    \n",
    "    # Retrieve results\n",
    "    results = [{\"text\": metadata[idx][\"text\"], \"id\": metadata[idx][\"id\"], \"distance\": distances[0][i]} \n",
    "               for i, idx in enumerate(indices[0])]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 7. Ask a question\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain the SAM algorithm?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mquery_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaiss_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 8. Print results\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "Cell \u001b[1;32mIn[7], line 49\u001b[0m, in \u001b[0;36mquery_index\u001b[1;34m(query, model, faiss_index, metadata, top_k)\u001b[0m\n\u001b[0;32m     46\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m faiss_index\u001b[38;5;241m.\u001b[39msearch(query_vector\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), top_k)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Retrieve results\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[1;32mIn[7], line 49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m distances, indices \u001b[38;5;241m=\u001b[39m faiss_index\u001b[38;5;241m.\u001b[39msearch(query_vector\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), top_k)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Retrieve results\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m results \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata[idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m: distances[\u001b[38;5;241m0\u001b[39m][i]} \n\u001b[0;32m     50\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# 7. Ask a question\n",
    "query = \"Explain the SAM algorithm?\"\n",
    "results = query_index(query, embed_model, faiss_index, metadata, top_k=3)\n",
    "\n",
    "# 8. Print results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i + 1}:\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"ID: {result['id']}\")\n",
    "    print(f\"Distance: {result['distance']}\\n\")\n",
    "# Updated result printing\n",
    "def print_results(results):\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Result {i + 1}:\")\n",
    "        # Clean and truncate the text for better readability\n",
    "        cleaned_text = \" \".join(result[\"text\"].split())  # Remove extra whitespaces and line breaks\n",
    "        truncated_text = cleaned_text[:500] + \"...\" if len(cleaned_text) > 500 else cleaned_text\n",
    "        print(f\"Text: {truncated_text}\")\n",
    "        print(f\"ID: {result['id']}\")\n",
    "        print(f\"Distance: {result['distance']:.4f}\\n\")  # Show distance with 4 decimal points\n",
    "\n",
    "# Print results using the updated function\n",
    "print_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_27524\\1242932843.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "c:\\Projects\\python\\mainenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Projects\\python\\mainenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sampling masks Automatically Generation ( SAM algorithm automically Generates masks from inputted prompt prompts image segmentation segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation Segmentation\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.core import Settings\n",
    "# ------------------------------------------------------\n",
    "# 1. Define Embedding Model (No More LangchainEmbedding)\n",
    "# ------------------------------------------------------\n",
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "#     # Optionally adjust parameters, e.g. batch_size\n",
    "#     # batch_size=16,\n",
    "# )\n",
    "embed_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# -----------------------------\n",
    "# 3. Load Documents (PDF/Text)\n",
    "# -----------------------------\n",
    "docs = SimpleDirectoryReader(\"./data/pdf_data\").load_data()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Build VectorStoreIndex\n",
    "#    Make sure to use 'embed_model=...' (NOT 'embedding=...')\n",
    "# -----------------------------\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs,\n",
    "    embed_model=embedding_model,   # <-- This ensures local embedding is used\n",
    "    chunk_size_limit=1024\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Create a Query Engine\n",
    "#    Pass the local HuggingFace LLM\n",
    "# -----------------------------\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Ask a Question\n",
    "# -----------------------------\n",
    "response = query_engine.query(\"What is attention mechanism?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\python\\mainenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Projects\\python\\mainenv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mechanism is a way of attention paid to certain parts of input when outputting something else entirely different from inputted parts attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention mechanism attention\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is attention mechanism?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
